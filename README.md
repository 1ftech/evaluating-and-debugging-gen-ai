# ğŸ§ª Evaluating and Debugging Generative AI

This project is based on the short course **"Evaluating and Debugging Generative AI"** by [DeepLearning.AI](https://www.deeplearning.ai/). The course introduces foundational skills and tools for managing and debugging large-scale generative AI projects, with a focus on using **Weights & Biases (W&B)** for experiment tracking and prompt management.

## ğŸš€ Course Objectives

By completing this project, you will gain hands-on experience in:

- ğŸ““ Instrumenting Jupyter notebooks for experiment tracking
- âš™ï¸ Managing hyperparameter configurations across multiple runs
- ğŸ“Š Logging and visualizing key run metrics (e.g., accuracy, loss)
- ğŸ§¬ Versioning datasets and models using artifacts
- âœ… Tracking experiment results for reproducibility and comparison
- ğŸ¤– Tracing prompts and responses to LLMs over time in complex workflows

## ğŸ› ï¸ Tools Used

- **Weights & Biases (W&B)** â€“ for experiment tracking, data/model versioning, and collaboration
- **Python** â€“ scripting and experimentation
- **Jupyter Notebooks** â€“ for development and iterative testing

## ğŸ§  What I Learned

- How to organize and manage generative AI experiments effectively
- Techniques for logging LLM inputs/outputs for better traceability
- Best practices for evaluating and debugging language model behavior
- Creating a repeatable, scalable ML workflow for generative AI applications


## ğŸ“ˆ Outcome

By the end of this project, you'll have a robust, traceable pipeline for developing, testing, and evaluating generative AI applicationsâ€”boosting both your productivity and model quality.

## ğŸ“š Reference

This project is inspired by the [Evaluating and Debugging Generative AI course](https://www.deeplearning.ai/short-courses/evaluating-and-debugging-generative-ai/) by Isa Fulford (OpenAI) and Andrew Ng (DeepLearning.AI).

---

Feel free to fork this repository, run the notebooks, and adapt the techniques to your own AI/ML projects!



