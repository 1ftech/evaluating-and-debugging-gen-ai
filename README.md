# 🧪 Evaluating and Debugging Generative AI

This project is based on the short course **"Evaluating and Debugging Generative AI"** by [DeepLearning.AI](https://www.deeplearning.ai/). The course introduces foundational skills and tools for managing and debugging large-scale generative AI projects, with a focus on using **Weights & Biases (W&B)** for experiment tracking and prompt management.

## 🚀 Course Objectives

By completing this project, you will gain hands-on experience in:

- 📓 Instrumenting Jupyter notebooks for experiment tracking
- ⚙️ Managing hyperparameter configurations across multiple runs
- 📊 Logging and visualizing key run metrics (e.g., accuracy, loss)
- 🧬 Versioning datasets and models using artifacts
- ✅ Tracking experiment results for reproducibility and comparison
- 🤖 Tracing prompts and responses to LLMs over time in complex workflows

## 🛠️ Tools Used

- **Weights & Biases (W&B)** – for experiment tracking, data/model versioning, and collaboration
- **Python** – scripting and experimentation
- **Jupyter Notebooks** – for development and iterative testing

## 🧠 What I Learned

- How to organize and manage generative AI experiments effectively
- Techniques for logging LLM inputs/outputs for better traceability
- Best practices for evaluating and debugging language model behavior
- Creating a repeatable, scalable ML workflow for generative AI applications


## 📈 Outcome

By the end of this project, you'll have a robust, traceable pipeline for developing, testing, and evaluating generative AI applications—boosting both your productivity and model quality.

## 📚 Reference

This project is inspired by the [Evaluating and Debugging Generative AI course](https://www.deeplearning.ai/short-courses/evaluating-and-debugging-generative-ai/) by Isa Fulford (OpenAI) and Andrew Ng (DeepLearning.AI).

---

Feel free to fork this repository, run the notebooks, and adapt the techniques to your own AI/ML projects!



